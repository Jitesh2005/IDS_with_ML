# Network Intrusion Detection System (NIDS)

A machine learning-based Network Intrusion Detection System using the NSL-KDD dataset. This project implements both supervised (XGBoost) and unsupervised (Autoencoder) approaches for detecting network attacks.

## ğŸ¯ Project Overview

This IDS system uses two complementary approaches:
- **XGBoost Classifier**: Supervised learning for binary classification (normal vs attack)
- **Autoencoder**: Unsupervised anomaly detection based on reconstruction error

## ğŸ“Š Dataset

**NSL-KDD Dataset** - An improved version of the KDD Cup 1999 dataset
- Training samples: KDDTrain+.txt
- Testing samples: KDDTest+.txt
- 41 features including protocol type, service, flag, and various network statistics
- Binary classification: Normal vs Attack traffic

## ğŸš€ Features

- Comprehensive data preprocessing pipeline
- One-hot encoding for categorical features
- Standard scaling for numerical features
- SMOTE for handling class imbalance
- XGBoost classifier with optimized hyperparameters
- Autoencoder-based anomaly detection
- Performance evaluation with confusion matrices
- Feature importance visualization

## ğŸ“ Project Structure

```
AI-ta2/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ KDDTrain+.txt          # Training dataset
â”‚   â””â”€â”€ KDDTest+.txt           # Testing dataset
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils.py               # Helper functions and data loading
â”‚   â”œâ”€â”€ data_prep.py           # Data preprocessing pipeline
â”‚   â”œâ”€â”€ train_xgb.py           # XGBoost model training
â”‚   â”œâ”€â”€ train_autoencoder.py   # Autoencoder training
â”‚   â””â”€â”€ evaluate.py            # Model evaluation
â”œâ”€â”€ models/                     # Saved models and preprocessors
â”‚   â”œâ”€â”€ xgb_model.joblib
â”‚   â”œâ”€â”€ ae_model.h5
â”‚   â”œâ”€â”€ scaler.joblib
â”‚   â””â”€â”€ ohe.joblib
â”œâ”€â”€ plots/                      # Visualization outputs
â”‚   â”œâ”€â”€ confusion_matrix_xgb.png
â”‚   â””â”€â”€ feature_importance_xgb.png
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- pip

### Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/network-ids.git
cd network-ids
```

2. Create a virtual environment:
```bash
python -m venv venv
```

3. Activate the virtual environment:
- **Windows:**
  ```bash
  venv\Scripts\activate
  ```
- **Linux/Mac:**
  ```bash
  source venv/bin/activate
  ```

4. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“¥ Dataset Setup

Download the NSL-KDD dataset and place the files in the `data/` directory:
- [NSL-KDD Dataset](https://www.unb.ca/cic/datasets/nsl.html)

Required files:
- `KDDTrain+.txt`
- `KDDTest+.txt`

## ğŸ“ Usage

### 1. Train XGBoost Model
```bash
python src/train_xgb.py
```
This will:
- Preprocess the data
- Apply SMOTE for class balancing
- Train the XGBoost classifier
- Save the model and generate visualizations

### 2. Train Autoencoder
```bash
python src/train_autoencoder.py
```
This will:
- Train the autoencoder on normal traffic only
- Save the trained model

### 3. Evaluate Models
```bash
python src/evaluate.py
```
This will:
- Load both trained models
- Evaluate on test data
- Display confusion matrices and classification reports

## ğŸ“ˆ Model Performance

### XGBoost Classifier
- **Architecture**: Gradient boosting with 250 estimators
- **Max Depth**: 12
- **Learning Rate**: 0.15
- **Evaluation Metrics**: Accuracy, Precision, Recall, F1-Score

### Autoencoder
- **Architecture**: 
  - Encoder: 128 â†’ 64 â†’ 32 neurons
  - Decoder: 32 â†’ 64 â†’ 128 neurons
- **Loss Function**: Mean Squared Error (MSE)
- **Anomaly Detection**: Threshold-based on reconstruction error

## ğŸ”§ Configuration

### XGBoost Hyperparameters
Edit `src/train_xgb.py`:
```python
model = XGBClassifier(
    n_estimators=250,
    max_depth=12,
    learning_rate=0.15,
    subsample=0.75,
    colsample_bytree=0.75,
    random_state=None  # For reproducibility, set to an integer
)
```

### Autoencoder Parameters
Edit `src/train_autoencoder.py`:
```python
ae.fit(X_train_norm, X_train_norm, 
       epochs=30, 
       batch_size=256, 
       validation_split=0.1)
```

## ğŸ“Š Outputs

### Saved Models
- `models/xgb_model.joblib` - Trained XGBoost classifier
- `models/ae_model.h5` - Trained autoencoder
- `models/scaler.joblib` - Feature scaler
- `models/ohe.joblib` - One-hot encoder

### Visualizations
- `plots/confusion_matrix_xgb.png` - Confusion matrix heatmap
- `plots/feature_importance_xgb.png` - Top 20 important features

## ğŸ§ª Testing

Run individual components:
```bash
# Test data preprocessing only
python src/data_prep.py

# Test utilities
python -c "from src.utils import load_nslkdd; print(load_nslkdd('data/KDDTrain+.txt').shape)"
```


## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- NSL-KDD Dataset creators
- XGBoost and TensorFlow/Keras communities
- scikit-learn and imbalanced-learn libraries
  

## ğŸ”® Future Improvements

- [ ] Add multi-class classification (DoS, Probe, R2L, U2R)
- [ ] Implement ensemble methods
- [ ] Add real-time detection capability
- [ ] Create web dashboard for monitoring
- [ ] Add more deep learning models (LSTM, CNN)
- [ ] Implement cross-validation
- [ ] Add hyperparameter tuning with GridSearch/RandomSearch

